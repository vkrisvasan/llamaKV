{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw9QVSMom/F323MxM8YGoC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6effd285418b4dc1b7e40a1fd4b62f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b598fcc41fb44f0696b8e36841414ae5",
              "IPY_MODEL_23ec2c52442346cd832ef791a3048447",
              "IPY_MODEL_d3df3a3fd1754830944929d129eb93b9"
            ],
            "layout": "IPY_MODEL_daecc6ae5ce04cc2961d21f762845b71"
          }
        },
        "b598fcc41fb44f0696b8e36841414ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deb058556cb543648039303bce0d3985",
            "placeholder": "​",
            "style": "IPY_MODEL_8c3307ba8cb6408cbfbb6af42dfe815d",
            "value": "Parsing nodes: 100%"
          }
        },
        "23ec2c52442346cd832ef791a3048447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c502323e6a6346f88df8eb5385be2ce4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b81ae05be5d94e35bdb59da36cb4c1fa",
            "value": 1
          }
        },
        "d3df3a3fd1754830944929d129eb93b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60946d8bfc0747799732387974104e61",
            "placeholder": "​",
            "style": "IPY_MODEL_bb5e62491b3748b49213dba1157feaad",
            "value": " 1/1 [00:00&lt;00:00, 10.05it/s]"
          }
        },
        "daecc6ae5ce04cc2961d21f762845b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb058556cb543648039303bce0d3985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3307ba8cb6408cbfbb6af42dfe815d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c502323e6a6346f88df8eb5385be2ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b81ae05be5d94e35bdb59da36cb4c1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60946d8bfc0747799732387974104e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb5e62491b3748b49213dba1157feaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "723365bdb84d46b19bd3de0c791c23b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b84ef5d252d94b98836d66471961a449",
              "IPY_MODEL_042ba2068d13404195d309b896d553d0",
              "IPY_MODEL_6ba2ba44aabe492894f670db74f12408"
            ],
            "layout": "IPY_MODEL_97865d7c7ad642b0a087c47a1fd8d5d1"
          }
        },
        "b84ef5d252d94b98836d66471961a449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e26e085f5858406eb8f9579e778638a0",
            "placeholder": "​",
            "style": "IPY_MODEL_085e10508f6e42e9ac5ab701a4cd6cd2",
            "value": "Parsing nodes: 100%"
          }
        },
        "042ba2068d13404195d309b896d553d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0b451f516f4c1aae90a2db47fe582e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85e5338193994208bd76ebb9db178d76",
            "value": 1
          }
        },
        "6ba2ba44aabe492894f670db74f12408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61c31186377741f58950dbd8e3c2cad4",
            "placeholder": "​",
            "style": "IPY_MODEL_d5df3f3e16eb4b03b76e85db3fb9e157",
            "value": " 1/1 [00:00&lt;00:00, 14.26it/s]"
          }
        },
        "97865d7c7ad642b0a087c47a1fd8d5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26e085f5858406eb8f9579e778638a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "085e10508f6e42e9ac5ab701a4cd6cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da0b451f516f4c1aae90a2db47fe582e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e5338193994208bd76ebb9db178d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61c31186377741f58950dbd8e3c2cad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5df3f3e16eb4b03b76e85db3fb9e157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7a7862046264043bf9aee763520a47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9444a2230143494f84c3282a2bd9877c",
              "IPY_MODEL_17c539bd90bb454b836cf28cd631a687",
              "IPY_MODEL_c26f00fb451941839035c77177b3230d"
            ],
            "layout": "IPY_MODEL_f56ec8db705e4e55a52146af3f16333b"
          }
        },
        "9444a2230143494f84c3282a2bd9877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c54c0b9e97483faff4d46509b51132",
            "placeholder": "​",
            "style": "IPY_MODEL_152bc3571db14cbe96811897e57250d2",
            "value": "Generating embeddings: 100%"
          }
        },
        "17c539bd90bb454b836cf28cd631a687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c0cefd95a04aacbe8667f36db4b9dd",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a83105f556b241e590193d0589428f7f",
            "value": 8
          }
        },
        "c26f00fb451941839035c77177b3230d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e99add10dfa94b919483200b7c78829d",
            "placeholder": "​",
            "style": "IPY_MODEL_615d6603507242cc92cc98dc1f3eb9bc",
            "value": " 8/8 [00:01&lt;00:00,  4.09it/s]"
          }
        },
        "f56ec8db705e4e55a52146af3f16333b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c54c0b9e97483faff4d46509b51132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152bc3571db14cbe96811897e57250d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2c0cefd95a04aacbe8667f36db4b9dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83105f556b241e590193d0589428f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e99add10dfa94b919483200b7c78829d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615d6603507242cc92cc98dc1f3eb9bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkrisvasan/llamaKV/blob/main/llamaindexYouTubekv_naiveRAGwithnoTracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EW6TTvklnoFJ"
      },
      "outputs": [],
      "source": [
        "#The code loads a Youtube transcript about Finetuning Llama,\n",
        "#splits it into smaller chunks, and uses it to create a question-answering system.\n",
        "#It leverages a large language model \"llama-3.1-8b-instant\" and\n",
        "#an embedding model \"sentence-transformers/all-MiniLM-L6-v2\" to understand and\n",
        "#uses VectorStoreIndex from llama_index, which saves the vector index and its associated data to a local file directory and\n",
        "#LlamaIndex acts as a bridge between the Youtube transcript by\n",
        "#ingesting, indexing and querying the data by using the capabilities of LLM and\n",
        "#respond to user queries about the transcript.\n",
        "\n",
        "# Install required packages\n",
        "!pip install youtube-transcript-api llama-index-readers-youtube-transcript llama-index llama-index-llms-groq groq llama-index-embeddings-huggingface -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules from llama_index\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    ServiceContext,\n",
        "    load_index_from_storage,\n",
        "    Settings\n",
        ")\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.groq import Groq"
      ],
      "metadata": {
        "id": "ES_U6bbrkhYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os and getpass for handling credentials\n",
        "import os\n",
        "import getpass\n",
        "# Prompt for credentials if not found in environment variables\n",
        "credential_names = [\"GROQ_API_KEY\"]\n",
        "for credential in credential_names:\n",
        "  if credential not in os.environ:\n",
        "    os.environ[credential]=getpass.getpass(\"Provide your...\" + credential)"
      ],
      "metadata": {
        "id": "a5upxJpqkloZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db638812-0092-44db-b117-1525f78d5de9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your...GROQ_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "iDaSIwxj-ApP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import YoutubeTranscriptReader and load data from YouTube links\n",
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(\n",
        "    #ytlinks=[\"https://www.youtube.com/watch?v=Kbk9BiPhm7o\"] #Elon Musk Nolan BCI NeuraLink Future of Humanity\n",
        "    ytlinks=[\"https://www.youtube.com/watch?v=pK8u4QfdLx0\"]  #\n",
        ")"
      ],
      "metadata": {
        "id": "46Aubpswnx11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SentenceSplitter for text splitting\n",
        "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "# Split documents into nodes\n",
        "nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6effd285418b4dc1b7e40a1fd4b62f9e",
            "b598fcc41fb44f0696b8e36841414ae5",
            "23ec2c52442346cd832ef791a3048447",
            "d3df3a3fd1754830944929d129eb93b9",
            "daecc6ae5ce04cc2961d21f762845b71",
            "deb058556cb543648039303bce0d3985",
            "8c3307ba8cb6408cbfbb6af42dfe815d",
            "c502323e6a6346f88df8eb5385be2ce4",
            "b81ae05be5d94e35bdb59da36cb4c1fa",
            "60946d8bfc0747799732387974104e61",
            "bb5e62491b3748b49213dba1157feaad"
          ]
        },
        "id": "uGgYHvW-k2Oa",
        "outputId": "004d0947-4913-4c84-dbfa-7b42b276e5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6effd285418b4dc1b7e40a1fd4b62f9e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information about loaded documents and nodes\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "print(f\"Split into {len(nodes)} nodes\")\n",
        "print(f\"nodes [0] {nodes[0].metadata} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QiYWkx6lDio",
        "outputId": "328ae3ca-4efa-425f-bbec-9f322f2d90d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 documents\n",
            "Split into 8 nodes\n",
            "nodes [0] {'video_id': 'pK8u4QfdLx0'} \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LLM and embedding model settings\n",
        "\n",
        "Settings.llm = Groq(model=\"llama-3.1-8b-instant\",api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    #model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "id": "1v1Ex4sunwf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create VectorStoreIndex from documents and persist to storage\n",
        "vector_index = VectorStoreIndex.from_documents(documents, show_progress=True, node_parser=nodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "723365bdb84d46b19bd3de0c791c23b0",
            "b84ef5d252d94b98836d66471961a449",
            "042ba2068d13404195d309b896d553d0",
            "6ba2ba44aabe492894f670db74f12408",
            "97865d7c7ad642b0a087c47a1fd8d5d1",
            "e26e085f5858406eb8f9579e778638a0",
            "085e10508f6e42e9ac5ab701a4cd6cd2",
            "da0b451f516f4c1aae90a2db47fe582e",
            "85e5338193994208bd76ebb9db178d76",
            "61c31186377741f58950dbd8e3c2cad4",
            "d5df3f3e16eb4b03b76e85db3fb9e157",
            "c7a7862046264043bf9aee763520a47d",
            "9444a2230143494f84c3282a2bd9877c",
            "17c539bd90bb454b836cf28cd631a687",
            "c26f00fb451941839035c77177b3230d",
            "f56ec8db705e4e55a52146af3f16333b",
            "83c54c0b9e97483faff4d46509b51132",
            "152bc3571db14cbe96811897e57250d2",
            "d2c0cefd95a04aacbe8667f36db4b9dd",
            "a83105f556b241e590193d0589428f7f",
            "e99add10dfa94b919483200b7c78829d",
            "615d6603507242cc92cc98dc1f3eb9bc"
          ]
        },
        "id": "jdOpOShOn7p-",
        "outputId": "1f28a599-8854-453b-a423-2e38072b3f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "723365bdb84d46b19bd3de0c791c23b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7a7862046264043bf9aee763520a47d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load index from storage and create query engine\n",
        "vector_index.storage_context.persist(persist_dir=\"./storage_mini\")\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage_mini\")\n",
        "\n",
        "index = load_index_from_storage(storage_context)\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "T0XTSW7qm9ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform queries and print responses\n",
        "query = \"summarise the document\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Y7nhkdBnYeG",
        "outputId": "75bbee23-efdf-484e-f41a-2858747a6826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document discusses fine-tuning language models (LLMs) to create engaging content and improve their performance on specific tasks. It mentions the importance of tailoring content generation to a particular audience or domain, and how fine-tuning can make a model's performance better. The author also shares their experience with implementing fine-tuning on the Llama free model using Google Colab, a free software that allows users to write and execute code in cells. They provide a step-by-step guide on how to prepare and load a range of quantized language models, including the new 15 trillion parameter model, and how to format data sets to train the model. The author emphasizes that fine-tuning is not limited to machine learning experts and encourages users to follow along and experiment with the process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Generate 5 difficult quiz questions with answer from the document \"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpNWWs79pFSq",
        "outputId": "7c636c5f-1999-4afb-98e1-734882053806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 difficult quiz questions with answers based on the provided context:\n",
            "\n",
            "1. What is the name of the Google software being used in the video, which splits the code into cells and is similar to a Jupyter Notebook?\n",
            "\n",
            "Answer: Google Collab.\n",
            "\n",
            "2. What is the name of the framework being used for fine-tuning the model, which allows for efficient updating of a fraction of the parameters and enhances training speed?\n",
            "\n",
            "Answer: Laura.\n",
            "\n",
            "3. What is the name of the dataset being used for data preparation, which has 50,000 rows and is loaded in VS Code?\n",
            "\n",
            "Answer: Alpaka dataset from YMA.\n",
            "\n",
            "4. What is the name of the model being used for fine-tuning, which is trained on 15 trillion tokens and is optimized for efficiency with 4-bit quantization?\n",
            "\n",
            "Answer: Llama 3 8B.\n",
            "\n",
            "5. What is the name of the feature in Google Collab that allows users to access a personalized AI strategy and future-proof themselves and their business?\n",
            "\n",
            "Answer: Personalized AI strategy (available to users who join the community during April).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Generate 5 hypothetical questions and answers from the document assuming that the scenario is applied to solve a retail business context\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfSa4tikpams",
        "outputId": "7bccf6a4-d406-48cf-ae7a-6b7ceb197b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are 5 hypothetical questions and answers from the document assuming that the scenario is applied to solve a retail business context:\n",
            "\n",
            "Q: How can we use fine-tuning to improve the performance of our language model in generating product descriptions for our e-commerce website?\n",
            "A: By fine-tuning our language model on a dataset of product descriptions from our specific retail domain, we can create a model that is tailored to our brand's tone and style, resulting in more engaging and effective product descriptions.\n",
            "\n",
            "Q: What are the benefits of using a 15-trillion token language model in a retail context, such as product recommendation or customer service chatbots?\n",
            "A: The 15-trillion token language model can provide more accurate and relevant product recommendations, as well as more effective customer service responses, due to its larger training dataset and optimized efficiency.\n",
            "\n",
            "Q: How can we use a UI-based system, such as GPT-4 or another open-source model, to integrate our fine-tuned language model into our retail website or mobile app?\n",
            "A: We can use a UI-based system to easily chat with our fine-tuned language model, allowing customers to interact with our product recommendations or customer service chatbots in a more intuitive and user-friendly way.\n",
            "\n",
            "Q: What are some potential use cases for fine-tuning a language model in a retail context, such as improving product search results or generating personalized marketing copy?\n",
            "A: Fine-tuning a language model can be used to improve product search results by creating a model that is tailored to our specific product catalog and customer search behavior. Additionally, fine-tuning can be used to generate personalized marketing copy that resonates with our target audience and drives sales.\n",
            "\n",
            "Q: How can we measure the effectiveness of our fine-tuned language model in a retail context, such as evaluating the impact on sales or customer satisfaction?\n",
            "A: We can measure the effectiveness of our fine-tuned language model by tracking key performance indicators (KPIs) such as sales lift, customer satisfaction ratings, and return on investment (ROI), and comparing them to a baseline or control group.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who are speakers\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z9n10Zsqi-y",
        "outputId": "c51b7b9c-f63e-4a2c-9dcb-978287e51d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The speaker in this video appears to be a machine learning expert or enthusiast who is explaining how to fine-tune a language model, specifically Llama 3, for a specific use case. They seem to be knowledgeable about the topic but also acknowledge their own limitations and encourage viewers to follow along and learn from the demonstration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"explain in detail about core of the finetuning\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7514f4c7-8143-4050-e1e5-67292926f0da",
        "id": "kTl6NHrv05SS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At the core of fine-tuning lies the concept of adapting a pre-trained language model to a specific task or domain by adjusting a small portion of its parameters. This process leverages the power of the pre-trained model, which has been trained on a massive dataset, to achieve improved performance on a smaller, more focused dataset.\n",
            "\n",
            "The pre-trained model's weights are updated incrementally using optimization algorithms, such as gradient descent, based on the new dataset. This incremental update process allows the model to learn from the new data and adjust its parameters to better fit the specific task or domain.\n",
            "\n",
            "The key to fine-tuning lies in the fact that only a small portion of the model's parameters are adjusted, rather than retraining the entire model from scratch. This approach is more data-efficient and cost-effective, as it leverages the pre-trained model's knowledge and avoids the need for extensive retraining.\n",
            "\n",
            "The core of fine-tuning can be broken down into several key components:\n",
            "\n",
            "1. **Pre-trained model**: A pre-trained language model, such as Llama 3, which has been trained on a massive dataset and has a large number of parameters.\n",
            "2. **Small dataset**: A smaller, more focused dataset that is specific to the task or domain being targeted.\n",
            "3. **Optimization algorithm**: An algorithm, such as gradient descent, that is used to update the pre-trained model's weights based on the new dataset.\n",
            "4. **Incremental update**: The process of updating the pre-trained model's weights incrementally, rather than retraining the entire model from scratch.\n",
            "5. **Parameter adjustment**: The process of adjusting a small portion of the pre-trained model's parameters to better fit the specific task or domain.\n",
            "\n",
            "By combining these components, fine-tuning allows for the creation of a customized language model that is tailored to a specific task or domain, while still leveraging the power of the pre-trained model. This approach has been shown to be highly effective in a variety of applications, including customer service transcripts, content generation, and domain-specific analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"explain top 10 impacts of finetuning - cover both 6 positive impacts and 4 negative impacts\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a01dae-c745-431b-c2b2-be19c64f4689",
        "id": "9TKPeACb3mq9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning can have a significant impact on the performance and usability of a language model. Here are the top 10 impacts of fine-tuning, covering both positive and negative effects:\n",
            "\n",
            "**Positive Impacts:**\n",
            "\n",
            "1. **Improved Accuracy**: Fine-tuning allows a language model to adapt to a specific task or domain, leading to improved accuracy and relevance of its outputs.\n",
            "2. **Enhanced Performance**: By adjusting a small portion of the model's parameters, fine-tuning can significantly enhance the model's performance on a specific task, often reaching 10 times better results.\n",
            "3. **Cost-Effectiveness**: Fine-tuning leverages the power of pre-trained language models, which can cost tens of millions of dollars to train, at a fraction of the cost, often just a few cents or dollars.\n",
            "4. **Data Efficiency**: Fine-tuning can achieve excellent results even with smaller data sets, making it an attractive option for those with limited data.\n",
            "5. **Customization**: Fine-tuning allows for customization of the language model to fit a specific use case, making it more tailored to the needs of the user.\n",
            "6. **Improved User Experience**: Fine-tuning can lead to improved user experience, as the language model can provide more relevant and accurate responses, reducing the need for manual intervention.\n",
            "\n",
            "**Negative Impacts:**\n",
            "\n",
            "1. **Overfitting**: Fine-tuning can lead to overfitting, where the model becomes too specialized to the training data and fails to generalize well to new, unseen data.\n",
            "2. **Increased Complexity**: Fine-tuning can add complexity to the model, making it more difficult to understand and maintain.\n",
            "3. **Dependence on Quality Data**: Fine-tuning relies on high-quality data, which can be time-consuming and expensive to prepare, and may not always be available.\n",
            "4. **Limited Transferability**: Fine-tuning can limit the transferability of the model to other tasks or domains, making it less versatile and more task-specific.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"explain llama 3 8b instrcut model\"\n",
        "resp = query_engine.query(query)\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPxdwxbvJlLS",
        "outputId": "dad91c73-4f77-4aaa-d06a-cf04d2ae4f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama 3 8B is a type of pre-trained language model that has been optimized for efficiency with a specific quantization technique, allowing it to be more compact and faster to run. This model is based on a transformer architecture, which is a popular choice for natural language processing tasks.\n",
            "\n",
            "The \"8B\" in Llama 3 8B refers to the number of parameters in the model, which is approximately 8 billion. This is a large number of parameters, indicating that the model has a high degree of complexity and is capable of learning a wide range of patterns and relationships in language.\n",
            "\n",
            "The instruction model aspect of Llama 3 8B suggests that it is designed to perform a specific set of tasks, such as answering questions, generating text, or completing tasks based on a set of instructions. This is in contrast to other types of language models that may be more general-purpose or focused on specific domains.\n",
            "\n",
            "Overall, Llama 3 8B is a powerful and efficient language model that is well-suited for a wide range of natural language processing tasks, particularly those that require a high degree of accuracy and flexibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the text in a paragraph format\n",
        "\n",
        "for doc in documents:\n",
        "  print(doc.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvruHqcwn0wA",
        "outputId": "b2100c48-cce4-4740-f449-82de61fb49d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my name is David Andre and in this video\n",
            "I'll teach you how to fine tune llama\n",
            "free so that it performs 10 times better\n",
            "for your specific use case let's start\n",
            "with what even is fine tuning and I made\n",
            "this explanation in plain English so\n",
            "that anybody can understand fine-tuning\n",
            "is adapting a pre-trained llm like gbd4\n",
            "or in this case Lama 3 to a specific\n",
            "task or domain it involves adjusting a\n",
            "small portion of the parameters on a\n",
            "more focused data set so you know when a\n",
            "new model releases what everybody needs\n",
            "to know is how many parameters it has we\n",
            "have llama 3 8B and always that number\n",
            "like 8B or 70b that's the number of\n",
            "parameters so we're adjusting just a\n",
            "small number of them to make it more\n",
            "focused on a specific thing fine tuning\n",
            "customizes the outputs to be more\n",
            "relevant and accurate for your use case\n",
            "here's the power of fine tuning cost\n",
            "Effectiveness it leverages the power of\n",
            "pre-trained llms which cost tens of\n",
            "millions of dollar if not hundreds of\n",
            "millions to train and we can just you\n",
            "know run a GPU for a few hours and fine\n",
            "tune something for I don't\n",
            "know like cents a few cents or few\n",
            "dollars at most which is just amazing it\n",
            "gives you improved performance because\n",
            "you can enhance the llm on your data set\n",
            "and improve accuracy for specific tasks\n",
            "and it it also is more data efficient\n",
            "you can achieve excellent results even\n",
            "with smaller data sets so you know maybe\n",
            "maybe even like 300 500 entries while\n",
            "you know llama 3 was trained on 15\n",
            "trillion tokens I don't know about you\n",
            "but I'm not have I don't have nearly as\n",
            "much data as Zak so that's why fine\n",
            "tuning is great for people like you and\n",
            "me so how does llm fine tuning actually\n",
            "work first you need to prepare your data\n",
            "set and this you know depending on how\n",
            "hardcore you want to go this can take\n",
            "anywhere from 20 minutes to a few hours\n",
            "to week\n",
            "potentially depends how far you want to\n",
            "take it so you create a smaller high\n",
            "quality data set tailored to your\n",
            "specific use case and label it\n",
            "appropriately which I'll teach you in a\n",
            "bit the pre-rain llms weights are\n",
            "updated incrementally using the\n",
            "optimization algorithms like grade in\n",
            "descent based on the new dat set so we\n",
            "can only fine-tune uh llms that we have\n",
            "access to the weights meaning open\n",
            "source open weights llms you cannot find\n",
            "you gbt 4 if you are not open AI open\n",
            "can do it obviously but me and you we\n",
            "probably don't have gp4 just laying on\n",
            "our\n",
            "computer then you Monitor and refine you\n",
            "evaluate the model's performance on a\n",
            "validation set preventing overfitting\n",
            "and guide adjustments now here are some\n",
            "real world use cases for fine tuning\n",
            "fine tuning and llm or customer service\n",
            "transcripts can create a chat bot like\n",
            "this one that can address issue in a way\n",
            "specific to the company so let's say you\n",
            "know you have a specific product very\n",
            "Niche that is not there is not much data\n",
            "about it on the internet and if somebody\n",
            "messages your customer support email you\n",
            "want your you know chatbot to respond in\n",
            "a specific way based on the information\n",
            "of your product and that data is\n",
            "proprietary it's private only you have\n",
            "it and you can find you an llm to\n",
            "respond based on that data so like\n",
            "technically if you have enough script\n",
            "you can find you an llm to respond like\n",
            "you and you know if you try sh GPT if\n",
            "you even give like sh GPT some writing\n",
            "and tell it continue in this writing\n",
            "style it's terrible so this is where\n",
            "fine tuning could be better tailored\n",
            "content generation so you can fine tune\n",
            "in llm on your posts and descriptions to\n",
            "create engaging summar or marketing copy\n",
            "again in your writing style tailored to\n",
            "your\n",
            "audience domain specific analysis so\n",
            "fine tuning llm on legal or medical text\n",
            "can make it much better for those\n",
            "specific Benchmark so and you might have\n",
            "a model that let's say it reaches 50 on\n",
            "some arbitrary Benchmark with fine\n",
            "tuning it can reach 70 or 80 now let's\n",
            "dive into how to actually implement this\n",
            "on Lama free so I created this Google\n",
            "collab well actually most of it was\n",
            "created by ansoff team A huge shout out\n",
            "to ansoff because they did all the heavy\n",
            "lifting so I'm going to also link their\n",
            "GitHub below now first off I added a\n",
            "component that's only available in April\n",
            "to the community so if you join during\n",
            "April you will get a personalized AI\n",
            "strategy to Future proof yourself and\n",
            "your business so if you want to be among\n",
            "people who are building the future if\n",
            "you want access to all the different\n",
            "courses modules and everything else in\n",
            "the community and to two we Rec calls\n",
            "then consider joining and especially if\n",
            "you want me to give you a personalized\n",
            "AI strategy to Future PR yourself so if\n",
            "that's interesting to you make sure to\n",
            "join the community it's the first link\n",
            "in the description now let's find youe\n",
            "Lama free shall we so first thing we\n",
            "check the GPU version available in the\n",
            "environment and install specific\n",
            "dependencies that are compatible with\n",
            "the detected GPU to prevent conflict so\n",
            "this is uh this cell by the way if you\n",
            "don't know how uh Google collab Works\n",
            "which is you know the software I'm using\n",
            "right now it's super simple it's\n",
            "basically um splitting the code into\n",
            "cells it's called The jupyter Notebook\n",
            "but it's like much more easier to see\n",
            "you can add text you can add graphics\n",
            "and it's great for like tutorials and\n",
            "explaining right so if you never use\n",
            "this it's great because it's free and\n",
            "Google actually gives you a GPU so you\n",
            "can use this T4 GPU to train this model\n",
            "for free and if you want faster you can\n",
            "obviously upgrade it right so I'm going\n",
            "to link this collab below the video as\n",
            "well so we run this cell which does what\n",
            "I just explained the next cell we need\n",
            "to prepare to load a range of quantied\n",
            "language models including the new 15\n",
            "trillion lvfree model so trained on 15\n",
            "trillion tokens and it's optimized for\n",
            "efficiency with forbit quantization I\n",
            "mean I'm not going to even pretend I\n",
            "know everything about fire tuning\n",
            "because I don't so if you know if um it\n",
            "seems like I have gaps in my knowledge\n",
            "it because it is I do have those gaps in\n",
            "my knowledge so I try to make it as\n",
            "simple as possible but if this proves\n",
            "something it proves that you don't have\n",
            "to be a machine learning expert to find\n",
            "your models so you know just follow\n",
            "along so here this is the max sequence\n",
            "length uh obviously 3 is up to 8,000 so\n",
            "I mean 2,000 is plenty for this\n",
            "demonstration but you can do anything\n",
            "you can do 4,000 or\n",
            "8,000 here use 4bit quantization to\n",
            "reduce memory usage but it can be false\n",
            "as well so here are the models we can\n",
            "see like we have mro 7B llama 2 which is\n",
            "the old one Gemma from Google but\n",
            "obviously we're interested in llama 3 8B\n",
            "and by the way we can also use llama\n",
            "370p if you want which obviously will\n",
            "take longer because uh it's a much\n",
            "bigger model so in that case you might\n",
            "uh want to buy the premium version of of\n",
            "collab or just wait for a while but yeah\n",
            "I mean uh everything is the same just\n",
            "here you would change the model to Lama\n",
            "fre 70p and if you want to use like a\n",
            "gated models from hugging face which\n",
            "gated means that you have to usually\n",
            "agree to some you know license or\n",
            "whatever then here just remove the\n",
            "comand and then put your hugging face\n",
            "token here super simple now by the way\n",
            "you always have to run this so what you\n",
            "do when you go to Google collab you\n",
            "click on run time and click run all that\n",
            "way all of the cells run but you can\n",
            "also do it one by one by clicking this\n",
            "button right here next to each cell and\n",
            "it needs to have this little tick green\n",
            "tick that way it was uh executed here\n",
            "it's not because I you know removed the\n",
            "I changed this so anytime you make any\n",
            "change it disappears but that doesn't\n",
            "matter it was still executed so it's\n",
            "stored in the run time next next up we\n",
            "integrate Laura again you don't have to\n",
            "understand what this is but it's\n",
            "basically um way of fine-tuning into our\n",
            "model which allows us to efficiently\n",
            "update just a fraction of the parameters\n",
            "enhancing training speed and reducing\n",
            "computation load so again we are not\n",
            "training the model from scratch we're\n",
            "just fine-tuning a few parameters for\n",
            "our specific use case and here you can\n",
            "change the r to Any number greater than\n",
            "zero 8 16 32 64 up to\n",
            "you and your goals would want to do with\n",
            "it by the way on SLO the reason I'm\n",
            "using it is because it's uh makes fine\n",
            "tuning much faster and consuming less\n",
            "memory so it's actually a great uh great\n",
            "framework for this data prep we now use\n",
            "the alpaka data set from yma which is\n",
            "this one which has 50,000 rows and I\n",
            "have it loaded in vs code here just that\n",
            "way you see how it looks like in Json\n",
            "formatting so you know it's a lot of\n",
            "lines because for everyone it's\n",
            "basically times five yeah so like 200\n",
            "50,000 uh lines and it's like every one\n",
            "every one of them has an\n",
            "instruction should probably Zoom it up\n",
            "Zoom it\n",
            "in so yeah every one entry has a\n",
            "instruction give fre tips for staying\n",
            "healie input this is not mandatory\n",
            "because instruction is already enough\n",
            "context and then output this is what the\n",
            "llm should say and you do this enough\n",
            "times and the llm you know learns it\n",
            "basically learns right so we you can see\n",
            "it probably better here uh and if you\n",
            "want to use your own data set you have\n",
            "to format it the same way so you know\n",
            "just having output input and\n",
            "instructions these three um parameters\n",
            "but yeah just look at this not all of\n",
            "them have the input which is fine I mean\n",
            "probably like 20% or 15% have the input\n",
            "and that's just extra context so yeah uh\n",
            "I'm also going to link this data set\n",
            "below but if you want your own data set\n",
            "which you know if you want your own use\n",
            "case just make sure to format it the\n",
            "same way so you know instruction some\n",
            "text input some extra context or empty\n",
            "and output how the model should respond\n",
            "and you know if you if you're getting\n",
            "creative you can definitely use llms to\n",
            "generate these large data sets much\n",
            "faster I mean maybe you create really\n",
            "like 20 high quality examples by hand\n",
            "and then you run a team of Agents um for\n",
            "creating that data set that can just you\n",
            "know use those 20 examples to create\n",
            "50,000 like in this data set but yeah\n",
            "that's a topic for a whole another video\n",
            "so if you want me to make a video on how\n",
            "to make data sets for fine tuning then\n",
            "let me know but let's go back to our\n",
            "collab so then we Define a system prompt\n",
            "which is you know custom instruction\n",
            "system prompt which you already know\n",
            "hopefully that formats tasks into\n",
            "instruction inputs and responses so this\n",
            "has to fit with our data set and we\n",
            "apply it to our data set for the model\n",
            "and we add the EOS token to Signal\n",
            "completion so this token right here here\n",
            "we Define it and here here we add it\n",
            "because without this the token\n",
            "generation continues forever so we don't\n",
            "want that obviously so let's look at the\n",
            "system prompt it's very simple it says\n",
            "below is a instruction that describes a\n",
            "task paired with an input that provides\n",
            "further context WR the response\n",
            "that appropriately completes the request\n",
            "and that's our system prompt and then we\n",
            "feed it the instruction the input and\n",
            "response and obviously you can change\n",
            "the system prompt if you\n",
            "want now train the model we do a 60 step\n",
            "uh we do only 60 steps here to speed\n",
            "things up um you can like this is\n",
            "obviously very small because it's not\n",
            "even one Epoch training Epoch so uh if\n",
            "you want to like actually use something\n",
            "for production or your business you\n",
            "probably want to train it for longer\n",
            "than 60 steps and I'm going to show you\n",
            "how how in this\n",
            "bit so if you if you do multiple EO you\n",
            "have to turn Max steps none so here okay\n",
            "number number of trained eox is not\n",
            "included in here so what you would do is\n",
            "you would copy this and you would go in\n",
            "here and look at the steps right so we\n",
            "have the steps here you would add this\n",
            "maybe you would do four or\n",
            "whatever however many you want the more\n",
            "the better but at a certain point it\n",
            "starts to not yield better result so max\n",
            "steps you have to change it to none\n",
            "right so this is 60 60 right now so you\n",
            "do none and this is where you would do\n",
            "like proper fine tuning but um you know\n",
            "I just add it that 604 demonstration\n",
            "that way it's faster and it still took\n",
            "like 8 minutes so I'm not going to\n",
            "replicate it I'm just going to show it\n",
            "everything but yeah basically um you\n",
            "know this is what you do you decide how\n",
            "many EO you want and then at this stage\n",
            "we confir configuring our models\n",
            "training setup where we Define things\n",
            "like badge\n",
            "size and learning rate to teach our\n",
            "model effectively with the data we've\n",
            "prep prepared so obviously you can like\n",
            "mess with stuff here um again I'm not\n",
            "going to PR pretend I understand\n",
            "everything but the main things are you\n",
            "know backing like this can make it five\n",
            "times faster for short sequences\n",
            "obviously the steps and the epox but um\n",
            "yeah I mean if you're confused something\n",
            "just take a screenshot boom like this\n",
            "and ask sh\n",
            "GPD now this is the current memory stats\n",
            "right so we're using the Tesla T4 GPU\n",
            "provided from Google for free and the\n",
            "max memory is 14\n",
            "GB and this is where the training begins\n",
            "this is the magical part right so here\n",
            "we do this line of code trainer stats uh\n",
            "trainer. train and this will give us the\n",
            "statistics as the model trains so again\n",
            "this is only 60\n",
            "steps which is um like zero EPO but yeah\n",
            "um you can see the training loss going\n",
            "down so like basically smaller number is\n",
            "better here so you can see like at the\n",
            "start we have 1.8 2 like 1.9 and then it\n",
            "quickly starts dropping to like 0.9 you\n",
            "know around 1 0.8 so it fluctuates a bit\n",
            "but it consistently go down 0.7 but you\n",
            "can see it's reaching like a as symt\n",
            "right obviously it's only 60 steps so\n",
            "really doesn't mean anything um but yeah\n",
            "like we ended up like 0.8 from like two\n",
            "so it shows you like if the model is\n",
            "actually\n",
            "improving and this took like 8 minutes\n",
            "you can see the stats here right so 476\n",
            "seconds almost exactly 8 minutes Peak\n",
            "Reserve memory was 8.9 GB and for\n",
            "training was 3.3 GB so not like this is\n",
            "the power of unso it's like really\n",
            "optimized for for this to use uh to run\n",
            "faster and to use less memory so that\n",
            "way we can find tune gpus for cheaper I\n",
            "mean you know I'm using a free T4 GPU\n",
            "from Google so it's free but it's faster\n",
            "like if you didn't use unso it would be\n",
            "a lot\n",
            "slower so okay so 60% of we used 60% of\n",
            "max memory so that's good because we\n",
            "didn't like hit the limit so we still\n",
            "have like 40% reserved and for training\n",
            "uh it was only 22% which is even better\n",
            "inference which is which means here we\n",
            "actually run our new model that we\n",
            "fine-tuned and okay so this data set is\n",
            "for like instructions and this is\n",
            "basically when you see a model that is\n",
            "like instruct at the end of it this is\n",
            "what they mean it's just trained on a\n",
            "large data set of instructions because\n",
            "usually the models are more for like\n",
            "chatting for text generation you know\n",
            "you give it some input and it's like\n",
            "gives you some output it's you know for\n",
            "more conversational here for\n",
            "instructions for instruct models is to\n",
            "follow instructions you give it a task\n",
            "and it completes it so like we can see\n",
            "it probably here in vs code like rewrite\n",
            "the sentence to change its meaning and\n",
            "then output the Fe\n",
            "escaped compar to dat sub so this is\n",
            "like all tasks it's all in instructions\n",
            "and then it shows how the model should\n",
            "do it\n",
            "so let's look at it right so now we've\n",
            "trained the model this took like 8\n",
            "minutes to do so all of you can do this\n",
            "the beauty of using a Google cloud is\n",
            "that obviously ly it doesn't matter what\n",
            "machine you have even if you have a\n",
            "terrible computer this will take the\n",
            "exact same time because you're using the\n",
            "GPU and\n",
            "Cloud so obviously here you can change\n",
            "your prompt I mean this is you know I\n",
            "changed the prompts here so this is my\n",
            "prompt uh but always make sure to leave\n",
            "the output blank so here the first one\n",
            "is the instruction then this is the\n",
            "input like the extra added context and\n",
            "the output leave it blank because the\n",
            "model will generate it right so list the\n",
            "prime numbers contained within this\n",
            "range and then the range is here in the\n",
            "input 1 to 50 and then the model our new\n",
            "findun the Lama 3 generates the output\n",
            "so let's look at this 2 3 5 7 11 13 17\n",
            "19 23 29 and just by looking at it uh\n",
            "you can see it's correct I mean none of\n",
            "these numbers are divisible so yeah this\n",
            "is correct all of them are prime\n",
            "numbers and also this is this is even\n",
            "better like I think this is much more\n",
            "visible using text streamer for\n",
            "continuous inference and I'm I'm just\n",
            "going to show it again by the way this\n",
            "is how it looks right so you have the\n",
            "instructions it's separated but that's\n",
            "not the main thing not only is it\n",
            "formatted better it's uh continuous\n",
            "inference so you can see the token\n",
            "generation token by token instead of\n",
            "waiting for the whole time so if I run\n",
            "this as you can see it waits and it\n",
            "generates it all at once right so boom\n",
            "it like appeared all at once so if you\n",
            "want to see it token by token this is\n",
            "much better right look at how fast it is\n",
            "this is the power of llama 3 8 billion a\n",
            "small model but a very capable model so\n",
            "um yeah Tech streamer is great for this\n",
            "and you can see it how it's generating\n",
            "the\n",
            "answer so yeah this is um the next\n",
            "prompt I Ed myself convert these binary\n",
            "numbers to decimal and then here and by\n",
            "the way again you can use these proms\n",
            "like example create like 20 30 by hand\n",
            "maybe and then you know feed this into\n",
            "CH GPD or your team of Agents something\n",
            "automated ideally or there is I think\n",
            "there's a service for like like\n",
            "reflection AI or something like that but\n",
            "yeah either way you can creating large\n",
            "data sets need to needs to be automated\n",
            "right so you cannot do that by hand but\n",
            "either way like you create something\n",
            "like this so uh these examples and then\n",
            "you would feed that into your own data\n",
            "set obviously relevant to your use case\n",
            "to your business and you just go crazy\n",
            "and create as many as as you possibly\n",
            "can\n",
            "so like really at least 1,000 like this\n",
            "is 50,000 and it's still probably could\n",
            "be larger so yeah I mean you have to\n",
            "again that's probably another video to\n",
            "build a team of agents to um generate\n",
            "data sets but yeah okay so here we give\n",
            "it um three different binary numbers and\n",
            "it tasks its tasks is to convert to\n",
            "decimal and as you can see it does it\n",
            "flawlessly I mean this is 10 13 15 that\n",
            "is correct so we have the model we\n",
            "tested it a bit with two prompts now\n",
            "it's time to to save it and because you\n",
            "know we spend all this time all this GPU\n",
            "power training it we don't want to go\n",
            "with to waste because if you restart the\n",
            "run time in Google collab um your model\n",
            "will disappear obviously you can run it\n",
            "again but then you have to wait again\n",
            "and you know maybe run out of the three\n",
            "GPU hours so to save the final model as\n",
            "Lura adapters we can either use hugging\n",
            "face push to hub for an online save if\n",
            "you wanted your model listed on hugging\n",
            "phase so hugging phase lists data assets\n",
            "and models it's probably the main two\n",
            "things it's used for so if you want your\n",
            "model shared then you would do that but\n",
            "if you want it just Sav on your computer\n",
            "do safe pre-train for a local\n",
            "save by the way this only saves the\n",
            "loraa adapters meaning um the like\n",
            "basically the things that were changed\n",
            "it doesn't save the entire model with\n",
            "the change parameters just the changes\n",
            "right so uh it's less memory and yeah\n",
            "just faster to save so but if you want\n",
            "to save the L adapters with the save\n",
            "model uh you can change this if you want\n",
            "to load the L adapters we saved for\n",
            "inference you would change this false to\n",
            "true so simply changing\n",
            "this and yeah this is the model name so\n",
            "obviously you can change this this is\n",
            "your model used for training uh here is\n",
            "just laa model but you can name it lava\n",
            "free I don't know copyrighting or Lama\n",
            "free uh medical diagnosis whatever your\n",
            "um use case is obviously and then\n",
            "um here the alpaka prompt so yeah this\n",
            "is the variable we declared earlier so\n",
            "this is the importance you can just go\n",
            "into the collab and try to running this\n",
            "cell you have to run the cells from\n",
            "above otherwise this will not work so\n",
            "whenever you're using a jupyter notebook\n",
            "such as Google collab always run all\n",
            "cells in order otherwise it will not\n",
            "work so\n",
            "yeah so this is the same uh format right\n",
            "from earlier inst ction input output at\n",
            "this point you should be familiar with\n",
            "this and that's just for this particular\n",
            "data set and for this style of prompting\n",
            "so if you have a different one then\n",
            "follow the different one so obviously\n",
            "here U what is the famous St Tower in\n",
            "Paris obviously it's Eiffel Tower blah\n",
            "blah blah it gives some extra info about\n",
            "it so you can also use hugging face Auto\n",
            "model for perf casual LM but ANS slof\n",
            "does not recommend this because it's a\n",
            "lot slower than ANS slof\n",
            "so yeah uh if possible use unslow for\n",
            "Speed and as the name suggests of you\n",
            "know unlove it's UNS slowing everything\n",
            "it's making everything two to five times\n",
            "faster so why not do that with 80% less\n",
            "memory so\n",
            "yeah okay and then we're preparing to\n",
            "save our trained model in a more compact\n",
            "format and then upload it into a cloud\n",
            "platform which allows for Less storage\n",
            "and comparation power so again like I'm\n",
            "not going to even pretend I understand\n",
            "everything because this is honestly\n",
            "stepping outside of my comfort zone but\n",
            "like just building this and doing this\n",
            "fine tuning taught me a lot so if you\n",
            "want more technical videos like this let\n",
            "me know next we're ready to compress our\n",
            "model using various quantization methods\n",
            "which means just you know making it\n",
            "easier to run or a machine so maybe you\n",
            "cannot like if you have a bad computer\n",
            "maybe you cannot run the full model but\n",
            "you definitely can run a quantized\n",
            "version of it it makes it leaner and\n",
            "then uh we upload it to the cloud for\n",
            "easy sh in this is what this piece of\n",
            "code does\n",
            "and so we use the model un. GF file or\n",
            "the quantise version so the Q4 means\n",
            "quanti in Lama CCP or if you want a UI\n",
            "based system which probably you do which\n",
            "is easier to use you can use like GPT\n",
            "for or or um is the other one is\n",
            "escaping me but yeah these are basically\n",
            "these USB system that you can use\n",
            "to or llm anything yeah I don't know if\n",
            "this supports it but yeah basically\n",
            "these uh these U Frameworks have a UI\n",
            "that's easy to chat with and you can use\n",
            "open source model there so if you do if\n",
            "you find this you can upload this to GPD\n",
            "for all and chat with your own model\n",
            "very easily and yeah that's it you know\n",
            "how to fine tune Lama 3 for your your\n",
            "own specific use case again I'm going to\n",
            "leave these resources below the video\n",
            "and if you have any questions regarding\n",
            "to anso join the their Discord so yeah\n",
            "that's it if you find this useful then\n",
            "please subscribe and again if you want\n",
            "during April which is what like eight\n",
            "days nine days left if you join the\n",
            "community you will get a personalized AI\n",
            "strategy to Future prooof yourself and\n",
            "your business so if that sounds valuable\n",
            "to you then make sure to join it's the\n",
            "first link in the description thank you\n",
            "for watching\n"
          ]
        }
      ]
    }
  ]
}