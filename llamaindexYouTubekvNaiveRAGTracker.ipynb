{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkrisvasan/llamaKV/blob/main/llamaindexYouTubekvNaiveRAGTracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "EW6TTvklnoFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca81cdd9-f6e7-438f-a408-8481a6d40759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\"\"\"This code sets up a question-answering system using a Youtube transcript about Neuralink.\n",
        "It leverages a large language model (\"llama-3.1-8b-instant\") and\n",
        "an embedding model (\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "to understand and respond to user queries about the transcript.\n",
        "The code also includes functionality to track\n",
        "the number of API calls made to the language model.\"\"\"\n",
        "# Request access to gated model https://huggingface.co/meta-llama/Meta-Llama-3-8B and check if we have access thru https://huggingface.co/settings/gated-repos\n",
        "# Install required packages\n",
        "!pip install llama-index llama-index-llms-groq groq llama-index-embeddings-huggingface llama-index-readers-youtube-transcript -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ES_U6bbrkhYD"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.core import (Settings,StorageContext,load_index_from_storage)\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5upxJpqkloZ",
        "outputId": "27547e78-c0a6-4c0f-8f00-7eac1b9937e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your...GROQ_API_KEY··········\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Import os and getpass for handling credentials\n",
        "import os\n",
        "import getpass\n",
        "# Prompt for credentials if not found in environment variables\n",
        "credential_names = [\"GROQ_API_KEY\"]\n",
        "for credential in credential_names:\n",
        "  if credential not in os.environ:\n",
        "    os.environ[credential]=getpass.getpass(\"Provide your...\" + credential)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "class TrackedLLM:\n",
        "    def __init__(self, llm):\n",
        "        print(\"TrackedLLM initialized\")\n",
        "        self.llm = llm\n",
        "        self.call_count = 0\n",
        "        self.total_tokens_in = 0\n",
        "        self.total_tokens_out = 0\n",
        "        # Use meta-llama/Meta-Llama-3-8B model's tokenizer after getting access to model\n",
        "        # Request access to gated model https://huggingface.co/meta-llama/Meta-Llama-3-8B and check if we have access thru https://huggingface.co/settings/gated-repos\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "\n",
        "    def __call__(self, input_text, query_engine_object):\n",
        "      try:\n",
        "        print(f\"Input text: {input_text}\")\n",
        "        print(\"TrackedLLM called\")\n",
        "        self.call_count += 1\n",
        "\n",
        "\n",
        "        # Count input tokens\n",
        "        input_tokens = self.tokenizer.encode(input_text)\n",
        "        print(f\"Input tokens: {input_tokens}\")\n",
        "        self.total_tokens_in += len(input_tokens)\n",
        "        print(f\"Total input tokens: {self.total_tokens_in}\")\n",
        "\n",
        "        # Use the correct method on the LLM instance\n",
        "        response = query_engine_object.query(input_text)\n",
        "        print(f\"Response: {response}\")\n",
        "\n",
        "        # Convert response to a string\n",
        "        if isinstance(response, dict):\n",
        "            output_text = response.get('text', '')\n",
        "        else:\n",
        "            output_text = str(response)\n",
        "\n",
        "        # Ensure output_text is a string before encoding\n",
        "        if not isinstance(output_text, str):\n",
        "            raise ValueError(\"Output text must be a string.\")\n",
        "\n",
        "        # Count output tokens\n",
        "        output_tokens = self.tokenizer.encode(output_text)\n",
        "        print(f\"Output tokens: {output_tokens}\")\n",
        "        self.total_tokens_out += len(output_tokens)\n",
        "        print(f\"Total output tokens: {self.total_tokens_out}\")\n",
        "\n",
        "        return response\n",
        "      except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        print(f\"Fetching attribute for LLM: {name}\")\n",
        "        return getattr(self.llm, name)\n",
        "\n",
        "class TrackedStorageContext:\n",
        "    def __init__(self, storage_context):\n",
        "        print(\"TrackedStorageContext initialized\")\n",
        "        self.storage_context = storage_context\n",
        "        self.persist_call_count = 0\n",
        "\n",
        "    def persist(self, *args, **kwargs):\n",
        "        print(\"TrackedStorageContext persist called\")\n",
        "        self.persist_call_count += 1\n",
        "        return self.storage_context.persist(*args, **kwargs)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        print(f\"Fetching attribute for storage: {name}\")\n",
        "        return getattr(self.storage_context, name)\n"
      ],
      "metadata": {
        "id": "KBVO4zgQZC2K"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.groq import Groq\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage\n",
        "\n",
        "# Initialize the original LLM\n",
        "llm = Groq(model=\"llama-3.1-8b-instant\", api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# Wrap the LLM with tracking\n",
        "tracked_llm = TrackedLLM(llm)\n",
        "Settings.llm = tracked_llm\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roDqjMizsZWT",
        "outputId": "c7c16bb4-f94e-4c43-8c4e-0b591caaa823"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrackedLLM initialized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the YouTube transcript loader and load documents\n",
        "links = [\"https://www.youtube.com/watch?v=Kbk9BiPhm7o\"]\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(ytlinks=links)\n",
        "print(\"Documents loaded\")\n",
        "\n",
        "# Create an index\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Initialize the original storage context\n",
        "storage_context = StorageContext.from_defaults()\n",
        "\n",
        "# Wrap the storage context with tracking\n",
        "tracked_storage_context = TrackedStorageContext(storage_context)\n",
        "\n",
        "# Persist the index with tracking\n",
        "index.storage_context.persist(persist_dir=\"./my_index_storage\")\n",
        "print(\"Index persisted\")\n",
        "\n",
        "# Load the index from storage with tracking\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./my_index_storage\")\n",
        "tracked_storage_context = TrackedStorageContext(storage_context)\n",
        "index = load_index_from_storage(storage_context=tracked_storage_context)\n",
        "print(\"Index loaded from storage\")\n",
        "\n",
        "# Create a query engine from the loaded index\n",
        "query_engine = index.as_query_engine()\n",
        "print(\"Query engine created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zM7mnHabMOr",
        "outputId": "976b511c-f8b5-49d1-efab-ce8e365995f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents loaded\n",
            "TrackedStorageContext initialized\n",
            "Index persisted\n",
            "TrackedStorageContext initialized\n",
            "Fetching attribute for storage: index_store\n",
            "Fetching attribute for storage: docstore\n",
            "Fetching attribute for storage: vector_store\n",
            "Fetching attribute for storage: graph_store\n",
            "Fetching attribute for storage: index_store\n",
            "Index loaded from storage\n",
            "Fetching attribute for LLM: metadata\n",
            "Query engine created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test direct LLM call\n",
        "response = tracked_llm(\"What is the content of the video?\", query_engine)\n",
        "print(\"Direct LLM response: \")\n",
        "print(response)\n",
        "# Print out the tracking results\n",
        "print(f\"LLM Call Count: {tracked_llm.call_count}\")\n",
        "print(f\"Total Input Tokens: {tracked_llm.total_tokens_in}\")\n",
        "print(f\"Total Output Tokens: {tracked_llm.total_tokens_out}\")\n",
        "print(f\"Persist Call Count: {tracked_storage_context.persist_call_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHlM1M0dQLWM",
        "outputId": "3f1c6609-7054-453e-8634-689ccb7a4ccb"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: What is the content of the video?\n",
            "TrackedLLM called\n",
            "Input tokens: [128000, 3923, 374, 279, 2262, 315, 279, 2835, 30]\n",
            "Total input tokens: 18\n",
            "Fetching attribute for LLM: predict\n",
            "Fetching attribute for LLM: __pydantic_validator__\n",
            "Response: The video appears to be a discussion about Brain-Computer Interface (BCI) technology, specifically focusing on the development and improvement of neural decoding and control systems. It involves a conversation about the capabilities and limitations of current BCI systems, including the use of neural implants and the importance of user interface design. The discussion also touches on the potential benefits of increasing the number of channels in a BCI system, including improved control quality and reliability.\n",
            "Output tokens: [128000, 791, 2835, 8111, 311, 387, 264, 10430, 922, 31417, 12, 38432, 20620, 320, 5002, 40, 8, 5557, 11, 11951, 21760, 389, 279, 4500, 323, 16048, 315, 30828, 48216, 323, 2585, 6067, 13, 1102, 18065, 264, 10652, 922, 279, 17357, 323, 9669, 315, 1510, 426, 11487, 6067, 11, 2737, 279, 1005, 315, 30828, 63284, 323, 279, 12939, 315, 1217, 3834, 2955, 13, 578, 10430, 1101, 29727, 389, 279, 4754, 7720, 315, 7859, 279, 1396, 315, 12006, 304, 264, 426, 11487, 1887, 11, 2737, 13241, 2585, 4367, 323, 31638, 13]\n",
            "Total output tokens: 200\n",
            "Direct LLM response: \n",
            "The video appears to be a discussion about Brain-Computer Interface (BCI) technology, specifically focusing on the development and improvement of neural decoding and control systems. It involves a conversation about the capabilities and limitations of current BCI systems, including the use of neural implants and the importance of user interface design. The discussion also touches on the potential benefits of increasing the number of channels in a BCI system, including improved control quality and reliability.\n",
            "LLM Call Count: 2\n",
            "Total Input Tokens: 18\n",
            "Total Output Tokens: 200\n",
            "Persist Call Count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start a simple chat loop\n",
        "while True:\n",
        "    query = input(\"Ask a question: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    # Test LLM and persistence tracking in a full script context\n",
        "    response = query_engine.query(query)\n",
        "    print(\"Query engine response: \")\n",
        "    print(response)\n",
        "    # Print out the tracking results\n",
        "    print(f\"LLM Call Count: {tracked_llm.call_count}\")\n",
        "    print(f\"Total Input Tokens: {tracked_llm.total_tokens_in}\")\n",
        "    print(f\"Total Output Tokens: {tracked_llm.total_tokens_out}\")\n",
        "    print(f\"Persist Call Count: {tracked_storage_context.persist_call_count}\")\n",
        "    # Persist the index after a query to check tracking\n",
        "    index.storage_context.persist(persist_dir=\"./my_index_storage\")\n",
        "    print(\"Index persisted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BYZtgR0cY6z",
        "outputId": "54cbfda3-cf04-4534-843e-94291a602556"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question: detail BCI\n",
            "Fetching attribute for LLM: predict\n",
            "Fetching attribute for LLM: __pydantic_validator__\n",
            "Query engine response: \n",
            "Brain-Computer Interfaces (BCIs) are systems that enable people to control devices or communicate with others using only their brain signals. The process of developing a BCI involves several key components:\n",
            "\n",
            "1. **Signal Acquisition**: This is the process of capturing brain signals from the user. This can be done using various techniques, such as electroencephalography (EEG), which measures electrical activity in the brain, or electrocorticography (ECoG), which measures electrical activity directly from the surface of the brain.\n",
            "\n",
            "2. **Signal Processing**: Once the brain signals are acquired, they need to be processed to extract meaningful information. This involves filtering out noise, amplifying the signals, and transforming them into a format that can be understood by the computer.\n",
            "\n",
            "3. **Decoding**: Decoding is the process of translating the brain signals into a specific action or command. This can be done using machine learning algorithms, such as deep neural networks, which can learn to recognize patterns in the brain signals and map them to specific actions.\n",
            "\n",
            "4. **User Interface**: The user interface is the part of the BCI that allows the user to interact with the device or communicate with others. This can be a simple keyboard or mouse interface, or a more complex interface that allows the user to control multiple devices or communicate with others in real-time.\n",
            "\n",
            "5. **Calibration**: Calibration is the process of training the BCI to recognize the user's brain signals and map them to specific actions. This can involve a series of exercises or tasks that the user performs while wearing the BCI, which helps the system to learn the user's brain patterns and improve its accuracy.\n",
            "\n",
            "6. **Feedback**: Feedback is an important component of BCIs, as it allows the user to see the results of their brain signals and adjust their behavior accordingly. This can be done using visual, auditory, or tactile feedback, depending on the specific application and user needs.\n",
            "\n",
            "BCIs have a wide range of potential applications, including:\n",
            "\n",
            "* **Assistive Technology**: BCIs can be used to help people with paralysis or other motor disorders control devices or communicate with others.\n",
            "* **Gaming**: BCIs can be used to create new types of games that are controlled by brain signals.\n",
            "* **Neuroscience Research**: BCIs can be used to study the brain and its functions in real-time.\n",
            "* **Prosthetics**: BCIs can be used to control prosthetic limbs or other devices that are controlled by brain signals.\n",
            "\n",
            "Overall, BCIs have the potential to revolutionize the way we interact with technology and communicate with others, and are an exciting area of research and development.\n",
            "LLM Call Count: 2\n",
            "Total Input Tokens: 18\n",
            "Total Output Tokens: 200\n",
            "Persist Call Count: 0\n",
            "TrackedStorageContext persist called\n",
            "Index persisted\n",
            "Ask a question: explain brain signals\n",
            "Fetching attribute for LLM: predict\n",
            "Fetching attribute for LLM: __pydantic_validator__\n",
            "Query engine response: \n",
            "Brain signals are a complex combination of electrical and chemical activity that occur within the brain. They can be thought of as a form of communication between different neurons and regions of the brain. When neurons fire, they generate electrical impulses that can be detected and recorded using various techniques.\n",
            "\n",
            "These electrical impulses can take the form of action potentials, which are brief, rapid changes in the electrical properties of the neuron. Action potentials can be thought of as a \"firing\" of the neuron, and they play a crucial role in the transmission of information within the brain.\n",
            "\n",
            "In addition to action potentials, brain signals can also involve oscillations and waves, such as gamma waves and beta waves. These oscillations can be thought of as a form of background noise or a \"hash\" of activity that occurs within the brain.\n",
            "\n",
            "The physics of brain signals is complex and involves the interaction of charged particles and electromagnetic fields. As you move away from the source of the signal, the strength of the signal can decrease due to diffusion and shielding effects.\n",
            "\n",
            "Despite the complexity of brain signals, researchers have made significant progress in understanding and recording them. Techniques such as electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) have allowed scientists to study brain activity in unprecedented detail.\n",
            "\n",
            "However, brain signals are not always easy to detect or interpret. Many neurons are \"silent\" or inactive, and only a small subset of neurons are active at any given time. This can make it challenging to understand the underlying mechanisms of brain function and behavior.\n",
            "\n",
            "Despite these challenges, researchers continue to make progress in understanding brain signals and their role in behavior and cognition. By developing new techniques and technologies, scientists hope to gain a deeper understanding of the complex processes that occur within the brain.\n",
            "LLM Call Count: 2\n",
            "Total Input Tokens: 18\n",
            "Total Output Tokens: 200\n",
            "Persist Call Count: 1\n",
            "TrackedStorageContext persist called\n",
            "Index persisted\n",
            "Ask a question: who is nolan\n",
            "Fetching attribute for LLM: predict\n",
            "Fetching attribute for LLM: __pydantic_validator__\n",
            "Query engine response: \n",
            "Nolan is someone the speaker knows quite well, and they have a personal relationship with him, including knowing his family.\n",
            "LLM Call Count: 2\n",
            "Total Input Tokens: 18\n",
            "Total Output Tokens: 200\n",
            "Persist Call Count: 2\n",
            "TrackedStorageContext persist called\n",
            "Index persisted\n",
            "Ask a question: what is problem nolan faced. explain in detail\n",
            "Fetching attribute for LLM: predict\n",
            "Fetching attribute for LLM: __pydantic_validator__\n",
            "Query engine response: \n",
            "Nolan faced a problem with cursor drift. This issue is related to the movement of the cursor or the pointer on a screen, which is not stable or consistent. In the context of neural interfaces, cursor drift can be a significant challenge, as it affects the accuracy and reliability of the system.\n",
            "\n",
            "The problem of cursor drift is likely caused by the movement of the brain's neural signals over time, which can lead to a gradual shift in the position of the cursor on the screen. This can be due to various factors, such as changes in the brain's neural activity, movement of the user's head or body, or other external factors.\n",
            "\n",
            "To address this problem, Nolan and his team are working on developing a system that can adapt to the changing neural signals and correct for cursor drift in real-time. This requires a sophisticated algorithm that can learn from the user's behavior and adjust the system's parameters accordingly.\n",
            "\n",
            "The challenge of cursor drift is not only technical but also user-centric. It requires a deep understanding of the user's needs and preferences, as well as the ability to design an intuitive and user-friendly interface that can accommodate the changing neural signals.\n",
            "\n",
            "In the context of the neural interface, cursor drift is a critical issue that needs to be addressed in order to achieve accurate and reliable results. By developing a system that can adapt to the changing neural signals and correct for cursor drift, Nolan and his team can improve the overall performance and usability of the neural interface, making it more effective for a wide range of applications.\n",
            "LLM Call Count: 2\n",
            "Total Input Tokens: 18\n",
            "Total Output Tokens: 200\n",
            "Persist Call Count: 3\n",
            "TrackedStorageContext persist called\n",
            "Index persisted\n",
            "Ask a question: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QSEKMgM0Zq4M"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAX6xoYk6t4LaV1Jnml4RC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}