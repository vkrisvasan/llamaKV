{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIDlR78c+wZQkot0w7kSBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkrisvasan/llamaKV/blob/main/llamaindexYouTubekv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EW6TTvklnoFJ",
        "outputId": "89c114de-1c9c-4846-f7f2-1ea7b36bd7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.7.4)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n",
            "Collecting llama-index-readers-youtube-transcript\n",
            "  Downloading llama_index_readers_youtube_transcript-0.1.4-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.1 (from llama-index-readers-youtube-transcript)\n",
            "  Downloading llama_index_core-0.10.61-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: youtube-transcript-api>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-youtube-transcript) (0.6.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.10.0)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.26.4)\n",
            "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading openai-1.39.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.31.0)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2024.5.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.7)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-youtube-transcript) (1.16.0)\n",
            "Downloading llama_index_readers_youtube_transcript-0.1.4-py3-none-any.whl (3.7 kB)\n",
            "Downloading llama_index_core-0.10.61-py3-none-any.whl (15.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading openai-1.39.0-py3-none-any.whl (336 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.7/336.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dirtyjson, tenacity, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index-core, llama-index-readers-youtube-transcript\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-core-0.10.61 llama-index-readers-youtube-transcript-0.1.4 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.39.0 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api\n",
        "!pip install llama-index-readers-youtube-transcript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.youtube_transcript import YoutubeTranscriptReader\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "documents = loader.load_data(\n",
        "    ytlinks=[\"https://www.youtube.com/watch?v=i3OYlaoj-BM\"]\n",
        ")"
      ],
      "metadata": {
        "id": "46Aubpswnx11"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print the text in a paragraph format\n",
        "\n",
        "for doc in documents:\n",
        "  print(doc.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvruHqcwn0wA",
        "outputId": "99e40d2e-685c-4224-a360-3e9015001186"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the imaginative laws scale visual\n",
            "recognition challenge was a\n",
            "world-changing competition\n",
            "that ran from around 2010 to 2017.\n",
            "during his time the competition acted as\n",
            "the place to go if you needed to find\n",
            "what the current state of the art was in\n",
            "image classification object localization\n",
            "object detection as well as that\n",
            "2012 onwards it really acted as the\n",
            "Catalyst of the explosion in deep\n",
            "learning researchers fine-tuned better\n",
            "performing computer vision models year\n",
            "on year but there was a unquestioned\n",
            "assumption causing problems\n",
            "it was assumed that every new task\n",
            "required model fine tuning this required\n",
            "a lot of data and a lot of data required\n",
            "a lot of capital and time it wasn't\n",
            "until recently that this assumption was\n",
            "challenged and proven wrong the\n",
            "astonishing rise of what are called\n",
            "multi-modal models has made the what was\n",
            "thought impossible very possible across\n",
            "various domains and tasks one of those\n",
            "is called zero shot object detection and\n",
            "localization now zero shot refers to\n",
            "taking a model and applying it to a new\n",
            "domain without ever fine-tuning it on\n",
            "data from that new domain so that means\n",
            "we can take a model who can it maybe it\n",
            "works in in one domain classification in\n",
            "one particular area on one data set and\n",
            "we can take that same model without any\n",
            "fine tuning and we can use it for object\n",
            "detection in a completely different\n",
            "domain without that model seeing any\n",
            "training data from that new domain so in\n",
            "this video we're going to explore how to\n",
            "use open ai's clip for zero shots object\n",
            "detection and localization let's begin\n",
            "with taking a quick look at image\n",
            "classification now image classification\n",
            "can kind of be seen as one of the\n",
            "simplest tasks in visual recognition and\n",
            "it's also the first step on the way to\n",
            "object detection at his core it's just\n",
            "assigning a categorical label to an\n",
            "image now moving on from image\n",
            "classification we have object\n",
            "localization object localization is\n",
            "image classification followed by the\n",
            "identification of where in the image the\n",
            "specific object actually is so we are\n",
            "localizing the the object now doing that\n",
            "where essentially just going to identify\n",
            "the coordinates on the image I going to\n",
            "return the typical approach to this is\n",
            "return an image where you have like a\n",
            "bounding box surrounding the the object\n",
            "that you are looking for and then we\n",
            "take this one step further to perform\n",
            "object detection with detection we are\n",
            "localizing multiple objects within the\n",
            "image or we have the capability to\n",
            "identify multiple objects within the\n",
            "image so in this example we have cat and\n",
            "a dog we would expect with object\n",
            "detection to identify both the cat and\n",
            "the dot in the case of us having\n",
            "multiple dolbs in this image almost Cuts\n",
            "in this image we would also expect the\n",
            "object detection algorithm to actually\n",
            "identify each one of those independently\n",
            "now in the past if we wanted to switch a\n",
            "model between anyone these tasks would\n",
            "have to fine-tune or more data you want\n",
            "to switch it to another domain we would\n",
            "have to also fine tune it on new data\n",
            "from that domain but that's not always\n",
            "the case with models like open ai's clip\n",
            "for performing each one of these tasks\n",
            "in a zero shot setting now open ai's\n",
            "clip is a multi-modal model that has\n",
            "been pre-trained on a huge number of\n",
            "text and image Pairs and it essentially\n",
            "works by identifying text and image\n",
            "pairs that have a similar meaning and\n",
            "placing them within a similar Vector\n",
            "space so every text and every image gets\n",
            "converted into a vector and they are\n",
            "placed in a shared Vector space and the\n",
            "vectors that appear close together they\n",
            "have a similar meaning now Clips very\n",
            "broad pre-training means that it can\n",
            "perform very effectively across a lot of\n",
            "different domains it's seen a lot of\n",
            "data and so it has a good understanding\n",
            "of all these different things and we can\n",
            "even adjust the task being performed\n",
            "with just a few code changes we don't\n",
            "actually have to is the model itself we\n",
            "just adjust the code around the model\n",
            "and that's very much thanks to Clips\n",
            "focus on sort of comparing these vectors\n",
            "so for example for classification we\n",
            "give clip a list of our plus labels and\n",
            "then we pass in the images and we just\n",
            "identify within that space where those\n",
            "images are with respect to those plus\n",
            "label vectors and which plus label is\n",
            "the most similar to our particular image\n",
            "and then that is our prediction so that\n",
            "most similar plus label\n",
            "that's our predated class now for\n",
            "objects localization we apply a very\n",
            "similar type of logic as before we\n",
            "create a class label but unlike before\n",
            "we don't need the entire image into clip\n",
            "to localize an object we have to break\n",
            "the image into patches we then pass a\n",
            "window over all of those patches moving\n",
            "across the entire image left to right\n",
            "top to bottom and we generate a image\n",
            "embedding for each of those windows and\n",
            "then we calculate the similarity between\n",
            "each one of those windows embedded by\n",
            "clip and the class label embedding\n",
            "returning a similarity score for every\n",
            "single patch now after calculating the\n",
            "similarity score for every single patch\n",
            "we use that to create almost like a map\n",
            "of relevance across the entire image and\n",
            "then we can use that map to identify the\n",
            "location of the object of interest and\n",
            "from that we will get something that's\n",
            "kind of like this so we have most of the\n",
            "image will be very dark and black that\n",
            "means as the object of interest is not\n",
            "in that space and then using that\n",
            "localization map we can create a more\n",
            "traditional bounding box visualization\n",
            "as well both of these visuals are\n",
            "capturing the same information we're\n",
            "just displaying it in a different way\n",
            "now there's also other approaches to\n",
            "this so I recently hosted a talk with\n",
            "what two two sets of people actually so\n",
            "Federico Bianchi from Stanford's NLP\n",
            "group and also Raphael pissoni and both\n",
            "of those have worked on a Italian clip\n",
            "project and part of that was performing\n",
            "object localization now to do that they\n",
            "use a slightly different approach what\n",
            "I'm going to demonstrate here and we can\n",
            "think of it as almost like the opposite\n",
            "so whereas we slide a window over the\n",
            "whole image they slide a black patch\n",
            "over the whole image which hides what is\n",
            "behind in that patch and then they feed\n",
            "the image into click and essentially as\n",
            "you slide the patch over the image you\n",
            "are hiding a part of the image and\n",
            "therefore if this similarity score drops\n",
            "when the patch is over a certain area\n",
            "you know that the object you're looking\n",
            "for is probably within that space and\n",
            "that's called the occlusion algorithm\n",
            "now moving on to object detection which\n",
            "is like the last level in these three\n",
            "tasks we will be identifying multiple\n",
            "objects now there's a very fine line\n",
            "between object localization and object\n",
            "detection but you can simply think of it\n",
            "as localization for multiple clusters\n",
            "and multiple objects with our cat and\n",
            "Butterfly image we will be searching for\n",
            "two objects a cat and a butterfly and\n",
            "with that we could draw a bounding box\n",
            "around both of those objects and\n",
            "essentially what we're doing now is\n",
            "using localization for a single object\n",
            "but then we're putting both of those\n",
            "together in a loop in our code and we're\n",
            "producing this object detection process\n",
            "now we've covered the idea behind my\n",
            "image classification onto object\n",
            "localization and object detection now\n",
            "let's have a look at how we actually\n",
            "Implement all of this now before we move\n",
            "on to any classification localization or\n",
            "detection task we need to have some data\n",
            "we're going to use a small demo data set\n",
            "called James Callum image text demo and\n",
            "we can download it like this so we're\n",
            "using hooking phase data sets here which\n",
            "we can pip install with Pip install\n",
            "data sets\n",
            "and this is the day so it's very small\n",
            "it's 21 text to image pairs okay one of\n",
            "those is the image you've already seen\n",
            "the cat with a butterfly landing on its\n",
            "nose very curious how they got that\n",
            "photo now after you've downloaded that\n",
            "data set\n",
            "we're going to be using this image here\n",
            "and what we want to do is not use the\n",
            "image file itself because at the moment\n",
            "it's a it's a pill python image object\n",
            "but instead we need to convert it into a\n",
            "canister now we're going to be using pi\n",
            "torch later on so what I want to do here\n",
            "is we're going to just transform the\n",
            "image into a tensor and we use toxin\n",
            "transforms use the typical pipeline tool\n",
            "in computer vision and we just use tube\n",
            "tensor okay and then we process our\n",
            "image through that Pipeline and then we\n",
            "can see that we get this okay so what\n",
            "are these values here we have the height\n",
            "of the image in pixels\n",
            "the width of the image in pixels and\n",
            "then also the three color channels red\n",
            "green and blue that make up the image\n",
            "now we need a slightly different format\n",
            "when we are processing everything one we\n",
            "need to add those patches and two we\n",
            "need to process it through a pie torch\n",
            "model and we also need the batch\n",
            "dimension for that so the first thing\n",
            "we're going to do is add the batch\n",
            "Dimension it's just a single image so we\n",
            "just have one in there but we we need\n",
            "that anyway and then we come down to\n",
            "here so this is where we're going to\n",
            "break the image into the patches okay\n",
            "each patch is going to be 256 dimensions\n",
            "in both height and width so the first\n",
            "thing we do here is unfold and we get\n",
            "this here okay there's two five six and\n",
            "there's 20. now the 20 is the height of\n",
            "the image in these 256 pixel patches\n",
            "and we can visualize that here\n",
            "all right so now we have all these kind\n",
            "of like slivers of the image that's just\n",
            "a vertical component of each patch\n",
            "and we use unfold again but in this time\n",
            "in the second Dimensions the targeting\n",
            "what was this Dimension here and we also\n",
            "get another 256 now we visualize that we\n",
            "get our four patches\n",
            "okay like this\n",
            "now if you just consider this here it's\n",
            "like if we look at this patch here it\n",
            "doesn't tell us anything about the image\n",
            "right and even when we're over cats\n",
            "these patches are way too small to\n",
            "actually tell us anything if clip is\n",
            "processing a single patch at a time\n",
            "it's probably not going to tell us\n",
            "anything maybe it could tell us that\n",
            "there's some hair in this patch or that\n",
            "there's an eye in this patch but beyond\n",
            "that it's not going to be very useful so\n",
            "rather than feeding single patches into\n",
            "clip what we do is actually feed a\n",
            "window a six by six patches or we can\n",
            "modify that value if we prefer and that\n",
            "just gives us a big patch to pass over\n",
            "to clip now the reason that we don't\n",
            "just do that from the start we don't\n",
            "just create these bigger patches to\n",
            "begin with is because when we're sliding\n",
            "through the image we want to have some\n",
            "degree of overlap between each patch\n",
            "okay so we create these smaller patches\n",
            "and then what we can do is actually\n",
            "slide across just one little patch at a\n",
            "time and we Define that using the stride\n",
            "variable so if we come down to here\n",
            "we have window we have stride\n",
            "remove this\n",
            "and here we go this is our code for\n",
            "going through the whole image creating a\n",
            "patch at every time step okay so we go\n",
            "for y and we go through the whole y-axis\n",
            "and then within that we're going across\n",
            "left to right with each step and we\n",
            "initialize a empty big patch array so\n",
            "this is our like the full window\n",
            "we got the current batch so okay let's\n",
            "say we start at zero zero X zero y zero\n",
            "we go from zero to six and zero to six\n",
            "here\n",
            "right so that gives us the very top left\n",
            "corner or window of the image and then\n",
            "we're literally going through and and\n",
            "just go processing all of that and you\n",
            "can see that happening here as wine eggs\n",
            "are increasing we're moving through that\n",
            "image and we're seeing each big patch\n",
            "from our image okay sliding across with\n",
            "a single small little patch at a time so\n",
            "that we don't miss any important\n",
            "information\n",
            "now this is how we're going to run\n",
            "through the whole image but before we do\n",
            "that we actually need clip so let's go\n",
            "ahead and actually initialize clip\n",
            "so to do that all we do is this so we're\n",
            "using hook and face Transformers which\n",
            "is using pi torch in the in the back\n",
            "there so we need the clip processor\n",
            "which is like a pre-processing pipeline\n",
            "for both text and images and then the\n",
            "actual model itself okay so we some\n",
            "model ID and we initialize both of those\n",
            "then what we want to do is move the\n",
            "model to advice if possible all right so\n",
            "we can use CPU but if you have a Kudo\n",
            "enabled GPU that will be here much\n",
            "faster so I'd recommend doing that if\n",
            "you can if not then you can use CPU it\n",
            "will be a bit slower but it will still\n",
            "run within a variable time frame so if\n",
            "I'm running this on my Mac I am using a\n",
            "CPU you can actually run this on MPS as\n",
            "well so you could change your device to\n",
            "MPS if you have an MPS enabled Apple\n",
            "silicon device\n",
            "so now returning to that process where\n",
            "we're going through each window within\n",
            "the image we're just going to add a\n",
            "little bit more logic so we are\n",
            "processing like we were before there's\n",
            "nothing different here we're creating\n",
            "that big patch and then what we do is\n",
            "process that big patch and process a\n",
            "text label okay so at the moment we're\n",
            "looking for a fluffy cat within this\n",
            "image so that is how we do this we're\n",
            "returning Pi torch tensors we also add\n",
            "padding here as well for the text\n",
            "although in this case I don't think we\n",
            "need it because we only have a single\n",
            "text item but we include that when we're\n",
            "using multiple text items later and then\n",
            "we calculate and retrieve the similarity\n",
            "score between them okay so if we pass\n",
            "both text and images through this\n",
            "processor we'll pass both into our\n",
            "inputs here and then we just calculate\n",
            "the or we extract the logits for each\n",
            "image and the item just converts that\n",
            "into a array of values or single value\n",
            "and then here we have those scores so\n",
            "what we're doing here is creating the\n",
            "what I earlier called like the relevance\n",
            "map or localization map throughout the\n",
            "whole image so for every window that we\n",
            "go through we're adding this score to\n",
            "every single patch or little patch\n",
            "within that window and what we're going\n",
            "to do or what we're going to find when\n",
            "we do that is that some patches will\n",
            "naturally have a high score than others\n",
            "because they are viewed more times right\n",
            "so if you think about the top left patch\n",
            "in the image that's only going to be\n",
            "viewed once whereas patches in the\n",
            "middle are going to be viewed many times\n",
            "because we'll have a sliding window\n",
            "going over there multiple times so what\n",
            "we also need to do is identify the\n",
            "number of runs that we perform or number\n",
            "of calculations that we perform within\n",
            "each one of those patches the reason we\n",
            "do that is so that we can take the\n",
            "average for each score based on the\n",
            "number of times that score has been\n",
            "capital related because here we're\n",
            "taking the total Awards scores and then\n",
            "we'll just take the average like so now\n",
            "the scores tensor is going to have a\n",
            "very smooth gradient of values from zero\n",
            "completely irrelevant to one now if you\n",
            "consider that we've been going over\n",
            "these scores multiple times it means\n",
            "that the object of interest is kind of\n",
            "like faded out of the window like over\n",
            "multiple steps so that means that the\n",
            "similarities score quite gradually faves\n",
            "out as you go away from the object which\n",
            "means that you don't really get very\n",
            "good localization if you use these\n",
            "scores directly so what you what we need\n",
            "to do is actually clip the lower scores\n",
            "down to zero so to do that what it is\n",
            "calculate the average of scores across\n",
            "the whole image we subtract that average\n",
            "from the current scores what that will\n",
            "do is push 50 of the scores below zero\n",
            "and then we click those scores so\n",
            "anything below zero becomes zero and we\n",
            "can do this multiple times okay one time\n",
            "is usually enough but you can do it\n",
            "multiple times to increase that effect\n",
            "of making the edge of this detected or\n",
            "localized area better defined and then\n",
            "after you've done that what we need to\n",
            "do is normalize those scores okay so we\n",
            "might have to do this a few times but\n",
            "everything's probably going to be within\n",
            "the range of like zero to 0.5 or 0 to\n",
            "0.2 so then we normalize those scores\n",
            "bring them back within the range of zero\n",
            "to one now to apply these scores to the\n",
            "patches we need to align their tenses\n",
            "because right now that they are not\n",
            "aligned okay for the scores we just we\n",
            "have like 20 by 13 tensor but for the\n",
            "patches we have the the batch Dimension\n",
            "there we have the 20 by 13 which we do\n",
            "want but then we have the three color\n",
            "channels and the two five six for each\n",
            "um set of pixels within each patch so we\n",
            "need to adjust that a little bit so we\n",
            "need to First remove the batch Dimension\n",
            "we do that by squeezing out the zero\n",
            "Dimension which is a batch Dimension and\n",
            "then we permute the different dimensions\n",
            "it's actually just moving them around in\n",
            "our patches in order to align them\n",
            "better with the score tensor dimensions\n",
            "and then all we do is multiply the\n",
            "patches by those scores that's pretty\n",
            "straightforward\n",
            "then we have to mute them again because\n",
            "if we want to visualize everything needs\n",
            "to be within a certain shape in order\n",
            "for us to visualize our matplotlib\n",
            "so we come down and the first thing you\n",
            "do is just get y next here so Y and X\n",
            "are the the patches\n",
            "see here this is y so the height of the\n",
            "image in patches and then 13 which is\n",
            "the width of the image in patches and we\n",
            "come down here and we can plot this okay\n",
            "and we get this it's pretty nice visual\n",
            "localizes the The Fluffy Cuts within\n",
            "that image now what's really interesting\n",
            "is if we just search for a cat we\n",
            "actually get a slightly different\n",
            "localization because here you can see\n",
            "it's kind of focusing a lot on the\n",
            "fluffy part of the cat so if we just\n",
            "search for a cat it would actually focus\n",
            "more on the head so we can really add\n",
            "nuanced information to these prompts and\n",
            "get a pretty nuanced response back\n",
            "now we can do the same for butterfly so\n",
            "we'll just throw all that code together\n",
            "this is just what we've done before we\n",
            "initialize scores and runs and we go\n",
            "process all of that the only thing we\n",
            "change here is the prompt we change it\n",
            "to a butterfly and if we go down and\n",
            "we're going to go down and down and\n",
            "visualize that we'll get this okay so\n",
            "again that's that's pretty cool we can\n",
            "see that it is identifying where in the\n",
            "image that butterfly actually is so that\n",
            "is the object localization set\n",
            "now I want to have a look at object\n",
            "detection which is essentially just\n",
            "taking the object localization and and\n",
            "wrapping some more code around it\n",
            "in order to look at these multiple\n",
            "objects rather than just one but to do\n",
            "that we can't really visualize in the\n",
            "same way that we've done here we're\n",
            "going to need a different type of\n",
            "visualization and that's where we have\n",
            "the bounding boxes so let's take a look\n",
            "at how we would do that so using the I\n",
            "think the butterfly examples of\n",
            "butterfly scores that we just calculated\n",
            "we're going to look at where those\n",
            "scores are higher than 0.5 now you can\n",
            "adjust this threshold based on you know\n",
            "what you find works best so we do this\n",
            "and what we'll get is a array of true\n",
            "and false values as to where the score\n",
            "was higher than 0.5 and not\n",
            "and then we detect where the non-zero\n",
            "values are in that array and what we do\n",
            "is get a load of X and Y values here\n",
            "so position three two we know that there\n",
            "is a score that is higher than 0.5 and\n",
            "we get three and two here so three is\n",
            "the row of the non-zero value and 2 is a\n",
            "column of the non-zero value so at row\n",
            "position three and column two we know\n",
            "that there is a non-zero value or a\n",
            "value or score that's higher than 0.5\n",
            "our threshold\n",
            "and put all that together we'll get\n",
            "something looks kind of like this so we\n",
            "already we kind of see that\n",
            "localization visual that we we just\n",
            "created\n",
            "and what we want to do is identify the\n",
            "bounding box that's just kind of\n",
            "surrounding those values okay so we know\n",
            "in terms of like a coordinate system we\n",
            "want one and three and four and ten to\n",
            "be included within that so what we do is\n",
            "find the corners from the detection\n",
            "array or or set of\n",
            "coordinates that we got before from NP\n",
            "non-zero and what we do is we just take\n",
            "the minimum X and Y values and maximum X\n",
            "and Y values and that will give us the\n",
            "corners of the box\n",
            "that's pretty simple to to calculate now\n",
            "when we get the maximum value what we\n",
            "want to do is because we basically we\n",
            "get in the position of the patch and the\n",
            "position of each patch we're essentially\n",
            "identifying the top left corner of each\n",
            "patch so when we're looking at the\n",
            "maximum value we actually want not the\n",
            "solved patch but the end of the patch\n",
            "okay so that's why we add that plus one\n",
            "here in order to get that\n",
            "the same for the x max value as well so\n",
            "that gives us a corner coordinates and\n",
            "then what we do is multiply those Corner\n",
            "coordinates by the patch size it's 256\n",
            "pixels and then we have the pixel\n",
            "positions of each one of those Corners\n",
            "because before we had the patch\n",
            "coordinates now we have the pixel\n",
            "coordinates which we can map directly\n",
            "onto the original image so we can see\n",
            "the minimum values here so we have for x\n",
            "and y two five six and a seven six eight\n",
            "and what we want to do because we're\n",
            "going to be using matplotlib patches and\n",
            "matplotlip patches expects the top left\n",
            "corner coordinates and the width and\n",
            "height of the bounding box that you want\n",
            "to create so we calculate the width and\n",
            "height and that's pretty simple it's\n",
            "just y Max minus y Min and X knives\n",
            "minus X min\n",
            "look at these\n",
            "and what we can do now is take the image\n",
            "we have to\n",
            "reshape it a little bit so we have to\n",
            "move the three color channels Dimension\n",
            "from the zeroth dimension to the final\n",
            "Dimension so we just do that here move\n",
            "axis\n",
            "and now we can plot that image okay so\n",
            "we showed that image with matplotlib\n",
            "and then we create a rectangle patch\n",
            "this is our bounding box okay so we pass\n",
            "X Min and Y Min that's the top left\n",
            "corner and then we also pass the width\n",
            "and height of what the boundary box\n",
            "should be\n",
            "and if we come down we get this visual\n",
            "okay so that's our bounding box\n",
            "visualization and with that it's not\n",
            "much further to create our object\n",
            "detection so let's have a look at how we\n",
            "do that now the logic for this is pretty\n",
            "much just a loop over what we've already\n",
            "done so I've put together a load of\n",
            "functions here which is essentially just\n",
            "what we've already gone through getting\n",
            "patches getting the the scores getting\n",
            "the the box and then the one thing that\n",
            "is new here is this detect function okay\n",
            "so we have detect that's going to get\n",
            "the the patches so it's going to take an\n",
            "image and it's going to split into those\n",
            "patches that we created we're going to\n",
            "convert the image into a format for\n",
            "displaying with matplotlib we did that\n",
            "before and we also initialized that plot\n",
            "and add our image to that plot and then\n",
            "what we do is we have for Loop and this\n",
            "for Loop goes through the image\n",
            "localization steps and bounding box\n",
            "steps that we just went through\n",
            "just multiple times okay so we have\n",
            "multiple primes and we want to do it\n",
            "multiple times so we calculate our\n",
            "similarity scores based on a specific\n",
            "prompt\n",
            "for all of our image Patches from that\n",
            "we get our our scores in that patch\n",
            "tensor format that we saw before\n",
            "and then what we do is we want to get\n",
            "the box based on a particular threshold\n",
            "so 0.5 like we used before you can see\n",
            "over there we have our patch size we\n",
            "just need to pass to that for a\n",
            "calculation of the or for the conversion\n",
            "and we have our patch size which we\n",
            "passed to that for the conversion from\n",
            "patch\n",
            "pixel from patch coordinates to pixel\n",
            "coordinates now we also have our scores\n",
            "and that will return the minimum X and Y\n",
            "coordinates and also width and height of\n",
            "the box\n",
            "we create the bounding box\n",
            "now we add that to the axis okay so now\n",
            "let's visualize all this see what we get\n",
            "so here I've used a slightly smaller\n",
            "window size before using six just to\n",
            "point out that you can change it and\n",
            "depending on your image it may be better\n",
            "to use a smaller or larger window\n",
            "and you can see so what we're doing here\n",
            "we've got a cat and a butterfly and you\n",
            "can see that we get we get a butterfly\n",
            "here and we get the cat here okay it's\n",
            "pretty cool and like I said with clip we\n",
            "can apply this object detection without\n",
            "fine tuning all we need to do is change\n",
            "these prompts here\n",
            "okay so it's it's really straightforward\n",
            "to modify this and move it to a new\n",
            "domain okay so that's it for this\n",
            "walkthrough of object localization and\n",
            "object detection with clip as I said I\n",
            "think zero shot object localization\n",
            "detection and even classification opens\n",
            "the doors to a lot of projects and use\n",
            "cases that were just not accessible\n",
            "before because time and capital\n",
            "constraints and now we can just use clip\n",
            "and get pretty impressive results very\n",
            "quickly all it requires is a bit of code\n",
            "changing here and there now I think clip\n",
            "is one part of a trend in multimodality\n",
            "that is kind of creating a more\n",
            "accessible ml that is less brittle like\n",
            "models were in the past that required a\n",
            "lot of fine tuning just to adapt to a\n",
            "slightly different domain and just more\n",
            "generally applicable which I thing is\n",
            "really exciting and I'm I'm it's really\n",
            "cool to see this sort of thing actually\n",
            "being used and to actually use it and\n",
            "just see how easy it is to use clip for\n",
            "so many different use cases and it work\n",
            "like incredibly easily so that's it for\n",
            "this video I hope it has been useful\n",
            "so thank you very much for watching and\n",
            "I will see you again in the next one bye\n"
          ]
        }
      ]
    }
  ]
}